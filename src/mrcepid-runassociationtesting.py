#!/usr/bin/env python
# mrcepid-runnassociationtesting 0.0.1
# Generated by dx-app-wizard.
#
# Author: Eugene Gardner (eugene.gardner at mrc.epid.cam.ac.uk)
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/

import dxpy
import csv
import subprocess
import tarfile
import pandas as pd
import statsmodels.api as sm
import numpy as np
import os
import math
import gzip
import runassociationtesting.covariate_processor
from os.path import exists
from concurrent import futures
from concurrent.futures import ThreadPoolExecutor
from pandas.core.series import Series


# This is to generate a global CHROMOSOMES variable for parallelisation
CHROMOSOMES = list(range(1,23)) # Is 0 based on the right coordinate...? (So does 1..22)
CHROMOSOMES.extend(['X'])
CHROMOSOMES = list(map(str, CHROMOSOMES))


# This function runs a command on an instance, either with or without calling the docker instance we downloaded
# By default, commands are not run via Docker, but can be changed by setting is_docker = True
# Also, by default, standard out is not saved, but can be modified with the 'stdout_file' parameter.
def run_cmd(cmd: str, is_docker: bool = False, stdout_file: str = None) -> None:

    # -v here mounts a local directory on an instance (in this case the home dir) to a directory internal to the
    # Docker instance named /test/. This allows us to run commands on files stored on the AWS instance within Docker.
    # This looks slightly different from other versions of this command I have written as I needed to write a custom
    # R script to run STAAR. That means we have multiple mounts here to enable this code to find the script.
    if is_docker:
        cmd = "docker run " \
              "-v /home/dnanexus:/test " \
              "-v /usr/bin/:/prog " \
              "egardner413/mrcepid-associationtesting " + cmd

    # Standard python calling external commands protocol
    proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = proc.communicate()
    if stdout_file is not None:
        with open(stdout_file, 'w') as stdout_writer:
            stdout_writer.write(stdout.decode('utf-8'))
        stdout_writer.close()

    # If the command doesn't work, print the error stream and close the AWS instance out with 'dxpy.AppError'
    if proc.returncode != 0:
        print("The following cmd failed:")
        print(cmd)
        print("STDERROR follows\n")
        print(stderr.decode('utf-8'))
        raise dxpy.AppError("Failed to run properly...")


# This function is slightly different that in other applets I have designed. This function handles ALL inputs rather
# than just external dependencies
def ingest_data(association_tarballs: str, phenofile: str, covarfile: str, inclusion_list: str, exclusion_list: str,
                run_bolt: bool, bgen_index: dict, transcript_index: dict, base_covariates: dict,
                bed_file: dict, fam_file: dict, bim_file: dict, low_MAC_list: dict,
                sparse_grm: dict, sparse_grm_sample: dict) -> dict:

    # Do easy single files first:
    # Get transcripts for easy annotation:
    dxpy.download_dxfile(dxpy.DXFile(transcript_index).get_id(), 'transcripts.tsv.gz')

    # Get covariate/phenotype data:
    dxpy.download_dxfile(dxpy.DXFile(base_covariates).get_id(), 'base_covariates.covariates')
    dxpy.download_dxfile(dxpy.DXFile(phenofile).get_id(), 'model_phenotypes.pheno')

    # Now grab all genetic data that I have in the folder /project_resources/genetics/
    os.mkdir("genetics/") # This is for legacy reasons to make sure all tests work...
    dxpy.download_dxfile(dxpy.DXFile(bed_file).get_id(), 'genetics/UKBB_450K_Autosomes_QCd.bed')
    dxpy.download_dxfile(dxpy.DXFile(bim_file).get_id(), 'genetics/UKBB_450K_Autosomes_QCd.bim')
    dxpy.download_dxfile(dxpy.DXFile(fam_file).get_id(), 'genetics/UKBB_450K_Autosomes_QCd.fam')
    dxpy.download_dxfile(dxpy.DXFile(low_MAC_list).get_id(), 'genetics/UKBB_450K_Autosomes_QCd.low_MAC.snplist')
    # This is the sparse matrix
    dxpy.download_dxfile(dxpy.DXFile(sparse_grm).get_id(), 'genetics/sparseGRM_450K_Autosomes_QCd_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseGRM.mtx')
    dxpy.download_dxfile(dxpy.DXFile(sparse_grm_sample).get_id(), 'genetics/sparseGRM_450K_Autosomes_QCd_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseGRM.mtx.sampleIDs.txt')

    # Now on to the more difficult stuff...
    # Need to grab the tarball file for associations...
    # This was generated by the applet mrcepid-collapsevariants
    # Ingest the list file into this AWS instance
    tarball_prefixes = []

    dxtarballs = dxpy.DXFile(association_tarballs)
    dxpy.download_dxfile(dxtarballs, "tarball_list.txt")
    with open("tarball_list.txt", "r") as tarball_reader:
        for association_tarball in tarball_reader:
            association_tarball = association_tarball.rstrip()
            tarball = dxpy.DXFile(association_tarball)
            tarball_name = tarball.describe()['name']
            dxpy.download_dxfile(tarball, tarball_name)

            # Need to get the prefix on the tarball to access resources within:
            # All files within SHOULD have the same prefix as this file
            tarball_prefix = tarball_name.rstrip('.tar.gz')
            tarball_prefixes.append(tarball_prefix)
            cmd = "tar -zxf " + tarball_name # unzip the tarball:
            run_cmd(cmd)

    # Grab the entire WES variant data in bgen format if we are running BOLT
    bgen_dict = {}
    if run_bolt:
        # Ingest the INDEX of bgen files:
        bgen_index = dxpy.DXFile(bgen_index)
        dxpy.download_dxfile(bgen_index.get_id(), "bgen_locs.tsv")
        # and load it into a dict:
        os.mkdir("filtered_bgen/") # For downloading later...
        bgen_index_csv = csv.DictReader(open("bgen_locs.tsv", "r"), delimiter="\t")
        for line in bgen_index_csv:
            bgen_dict[line['chrom']] = {'index': line['bgen_index_dxid'],
                                         'sample': line['sample_dxid'],
                                         'bgen': line['bgen_dxid'],
                                         'vep': line['vep_dxid']}
            
    # Check if additional covariates were provided:
    additional_covariates_found = False # default is we DID NOT find a file
    if covarfile is not None:
        covarfile = dxpy.DXFile(covarfile)
        dxpy.download_dxfile(covarfile, 'additional_covariates.covariates')
        additional_covariates_found = True

    # Get inclusion/exclusion sample lists
    inclusion_found = False # default is we DID NOT find a file
    if inclusion_list is not None:
        inclusion_list = dxpy.DXFile(inclusion_list)
        dxpy.download_dxfile(inclusion_list, 'INCLUSION.lst')
        inclusion_found = True
    exclusion_found = False # default is we DID NOT find a file
    if exclusion_list is not None:
        exclusion_list = dxpy.DXFile(exclusion_list)
        dxpy.download_dxfile(exclusion_list, 'EXCLUSION.lst')
        exclusion_found = True

    return {'tarball_prefixes': tarball_prefixes,
            'inclusion_found': inclusion_found,
            'exclusion_found': exclusion_found,
            'additional_covariates_found': additional_covariates_found,
            'bgen_dict': bgen_dict}


# Three steps here:
# 1. Get individuals we plan to include
# 2. Exclude individuals not wanted in the analysis
# 3. Get individuals that are POSSIBLE to include (they actually have WES) and only keep 'include' samples
def select_individuals(inclusion_found: bool, exclusion_found: bool) -> set:

    # Get a list of individuals that we ARE going to use
    include_samples = set()
    if inclusion_found is True:
        inclusion_file = open('INCLUSION.lst', 'r')
        for indv in inclusion_file:
            indv = indv.rstrip()
            include_samples.add(indv)

    # Get a list of individuals that we ARE NOT going to use
    exclude_samples = set()
    if exclusion_found is True:
        exclude_file = open('EXCLUSION.lst', 'r')
        for indv in exclude_file:
            indv = indv.rstrip()
            exclude_samples.add(indv)

    # Get individuals with genetic data
    # Remember! the genetic data has already been filtered to individuals with WES data.
    genetics_fam_file = open('genetics/UKBB_450K_Autosomes_QCd.fam', 'r')
    genetics_samples = set()
    for line in genetics_fam_file:
        line = line.rstrip()
        fields = line.split()
        eid = fields[0]
        if inclusion_found == False and exclusion_found == False:
            genetics_samples.add(eid)
        elif inclusion_found == False and exclusion_found == True:
            if eid not in exclude_samples:
                genetics_samples.add(eid)
        elif inclusion_found == True and exclusion_found == False:
            if eid in include_samples:
                genetics_samples.add(eid)
        else:
            if eid in include_samples and eid not in exclude_samples:
                genetics_samples.add(eid)

    print("Total samples after inclusion/exclusion lists applied: %i" % len(genetics_samples))
    return genetics_samples


# This is a helper function for 'create_covariate_file()' that processes the phenotype file
def process_phenotype() -> tuple:
    # Need to go through phenofile first and injest into a dictionary and get the name of the phenofield:
    dialect = csv.Sniffer().sniff(open('model_phenotypes.pheno', 'r').readline(), delimiters=[' ','\t'])
    pheno_reader = csv.DictReader(open('model_phenotypes.pheno', 'r'), delimiter=dialect.delimiter, skipinitialspace=True)
    field_names = pheno_reader.fieldnames
    if len(field_names) != 3:
        raise RuntimeError("Pheno file has more than three columns!")
    elif "FID" not in field_names and "IID" not in field_names:
        raise RuntimeError("Pheno file has column names other than FID/IID/Phenotype!")

    for field in field_names:
        if field != "FID" and field != "IID":
            pheno_name = field

    phenotypes = {}
    for indv in pheno_reader:
        # Will spit out an error if a given sample does not have data
        if indv[pheno_name] is None:
            raise dxpy.AppError("Phenotype file has blank lines!")
        # Exclude individuals that have missing data (NA/NAN)
        elif indv[pheno_name].lower() != "na" and indv[pheno_name].lower() != "nan" and indv[pheno_name].lower() != "":
            phenotypes[indv['FID']] = indv[pheno_name]

    return phenotypes, pheno_name


# This is a helper function for 'create_covariate_file()' that processes requested additional phenotypes
def process_additional_covariates(additional_covariates_found: bool, categorical_covariates: str, quantitative_covariates: str) -> tuple:

    if additional_covariates_found:
        dialect = csv.Sniffer().sniff(open('additional_covariates.covariates', 'r').readline(), delimiters=[' ','\t'])
        additional_covar_reader = csv.DictReader(open('additional_covariates.covariates', 'r'), delimiter=dialect.delimiter, skipinitialspace=True)
        field_names = list.copy(additional_covar_reader.fieldnames)

        # make sure the sample ID field is here and remove it from 'field_names' to help with iteration
        if 'FID' not in field_names and 'IID' not in field_names:
            raise dxpy.AppError("FID & IID column not found in provided covariates file!")
        else:
            field_names.remove('FID')
            field_names.remove('IID')

        # Now process & check the categorical/quantitative covariates lists and match it to field_names:
        found_categorical_covariates = []
        if categorical_covariates is not None:
            categorical_covariates = categorical_covariates.split(',')
            for covar in categorical_covariates:
                if covar in field_names:
                    found_categorical_covariates.append(covar)
                else:
                    print("Provided categorical covariate %s not found in additional covariates file..." % covar)

        found_quantitative_covariates = []
        if quantitative_covariates is not None:
            quantitative_covariates = quantitative_covariates.split(',')
            for covar in quantitative_covariates:
                if covar in field_names:
                    found_quantitative_covariates.append(covar)
                else:
                    print("Provided quantitative covariate %s not found in additional covariates file..." % covar)

        # Throw an error if user provided covariates but none were found
        if (len(found_categorical_covariates) + len(found_quantitative_covariates)) == 0:
            raise dxpy.AppError('Additional covariate file provided but no additional covariates found based on covariate names provided...')

        add_covars = {}
        for sample in additional_covar_reader:
            # First check no NAs/Blanks exist
            all_covars_found = True
            sample_dict = {}
            for field_name in (found_quantitative_covariates + found_categorical_covariates):
                if sample[field_name].lower() == "na" or sample[field_name].lower() == "nan" or sample[field_name].lower() == "":
                    all_covars_found = False
                else:
                    sample_dict[field_name] = sample[field_name]

            if all_covars_found == True:
                add_covars[sample['IID']] = sample_dict

        return add_covars, found_quantitative_covariates, found_categorical_covariates
    else:
        return {}, [], []


# Do covariate processing and sample inclusion/exclusion
def create_covariate_file(sex: int, genetics_samples: set, additional_covariates_found: bool, categorical_covariates: str,
                          quantitative_covariates: str) -> dict:

    # Process the phenotype:
    phenotypes, pheno_name = process_phenotype()

    # Process additional covariates (check if requested in the function):
    add_covars, found_quantitative_covariates, found_categorical_covariates = process_additional_covariates(additional_covariates_found,
                                                                                                            categorical_covariates,
                                                                                                            quantitative_covariates)
    # Read the base covariates into this code that we want to analyse:
    # Formatting is weird to fit with other printing below...
    print("Default covariates included in model                        :")
    print("    Quantitative                                            : age, age^2, PC1..PC10")
    if (sex == 2):
        print("    Categorical                                             : sex, WES_batch")
    else:
        print("    Categorical                                             : WES_batch")
    if additional_covariates_found:
        print("Number of individuals with non-null additional covariates   : %i" % len(add_covars))
        print("Additional covariates included in model                     :")
        if len(found_quantitative_covariates) > 0:
            print("    Quantitative                                            : " + ', '.join(found_quantitative_covariates))
        else:
            print("    Quantitative                                            : NULL")
        if len(found_categorical_covariates) > 0:
            print("    Categorical                                             : " + ', '.join(found_categorical_covariates))
        else:
            print("    Categorical                                             : NULL")
    else:
        print("No additional covariates provided/found beyond defaults...")

    base_covar_reader = csv.DictReader(open('base_covariates.covariates', 'r'), delimiter="\t")
    indv_written = 0 # Just to count the number of samples we will analyse
    formatted_combo_file = open('phenotypes_covariates.formatted.txt', 'w') # SAIGE needs a combo file

    write_fields = ["FID", "IID"]
    write_fields = write_fields + ["PC%s" % (x) for x in range(1,41)]
    write_fields = write_fields + ["age", "age_squared", "sex", "wes_batch"]
    write_fields = write_fields + [pheno_name]
    # This doesn't matter to python if we didn't find additional covariates. A list of len() == 0 does not lengthen
    # the target list (e.g. 'write_fields')
    write_fields = write_fields + found_quantitative_covariates + found_categorical_covariates

    combo_writer = csv.DictWriter(formatted_combo_file,
                                  fieldnames = write_fields,
                                  quoting = csv.QUOTE_NONE,
                                  delimiter = " ",
                                  extrasaction='ignore')
    combo_writer.writeheader()

    # Need a list of included individuals ONLY:
    include_samples = open('SAMPLES_Include.txt', 'w')
    num_all_samples = 0
    na_pheno_samples = 0 # for checking number of individuals missing phenotype information
    for indv in base_covar_reader:
        if indv['22001-0.0'] != "NA": # need to exclude blank row individuals, eid is normally the only thing that shows up, so filter on sex
            indv_writer = {'FID': indv['eid'],
                           'IID': indv['eid']}
            for PC in range(1,41):
                old_PC = "22009-0.%s" % (PC)
                new_pc = "PC%s" % (PC)
                indv_writer[new_pc] = indv[old_PC]
            indv_writer['age'] = int(indv['21003-0.0'])
            indv_writer['age_squared'] = indv_writer['age']**2
            indv_writer['sex'] = int(indv['22001-0.0'])
            indv_writer['wes_batch'] = indv['wes.batch']

            # Check if we found additional covariates and make sure this sample has non-null values
            write_sample = False
            if len(add_covars) > 0:
                if indv['eid'] in add_covars:
                    write_sample = True
                    for covariate in add_covars[indv['eid']]:
                        indv_writer[covariate] = add_covars[indv['eid']][covariate]
            else:
                write_sample = True

            # exclude based on sex-specific analysis if required:
            if indv['eid'] in genetics_samples:
                num_all_samples += 1
                if indv['eid'] in phenotypes and write_sample:
                    indv_writer[pheno_name] = phenotypes[indv['eid']]
                    if sex == 2:
                        indv_written += 1
                        combo_writer.writerow(indv_writer)
                        include_samples.write(indv['eid'] + "\n")
                    elif sex == indv_writer['sex']:
                        indv_written += 1
                        combo_writer.writerow(indv_writer)
                        include_samples.write(indv['eid'] + "\n")
                else:
                    na_pheno_samples += 1

    formatted_combo_file.close()
    include_samples.close()

    # Generate a plink file to use that only has included individuals:
    cmd = "plink2 " \
          "--bfile /test/genetics/UKBB_450K_Autosomes_QCd --make-bed --keep-fam /test/SAMPLES_Include.txt " \
          "--out /test/genetics/UKBB_450K_Autosomes_QCd_WBA"
    run_cmd(cmd, True)

    # I have to do this to recover the sample information from plink
    cmd = "docker run -v /home/dnanexus/:/test/ egardner413/mrcepid-associationtesting plink2 " \
          "--bfile /test/genetics/UKBB_450K_Autosomes_QCd_WBA " \
          "--validate | grep samples"
    proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = proc.communicate()

    # Print to ensure that total number of individuals is consistent between genetic and covariate/phenotype data
    print("Samples with covariates after include/exclude lists applied : %i" % num_all_samples)
    print("Number of individuals with NaN/NA phenotype information     : %i" % na_pheno_samples)
    print("Number of individuals written to covariate/pheno file       : %i" % indv_written)
    print("Plink individuals written                                   : %s" % stdout.decode('utf-8').rstrip())

    # Return the phenotype name and additional covariates (if any) so we can use it later
    return {'pheno_name': pheno_name,
            'quant_covars': found_quantitative_covariates,
            'cat_covars': found_categorical_covariates}


# This handles processing of mask and whole-exome bgen files for input into BOLT
def process_bolt_file(tarball_prefix: str, chromosome: str) -> None:

    # Do the mask first...
    # We need to modify the bgen file to have an alternate name for IDing masks
    cmd = "plink2 --threads 4 --bgen /test/" + tarball_prefix + "." + chromosome + ".BOLT.bgen 'ref-last' " \
            "--out /test/" + tarball_prefix + "." + chromosome + " " \
            "--make-just-pvar"
    run_cmd(cmd, True)

    with open(tarball_prefix + "." + chromosome + ".fixer", 'w') as fix_writer:
        pvar_reader = csv.DictReader(open(tarball_prefix + "." + chromosome + ".pvar", 'r'), delimiter = "\t")
        for id in pvar_reader:
            fix_writer.write("%s %s\n" % (id['ID'], id['ID'] + "-" + tarball_prefix))
        fix_writer.close()

    cmd = "plink2 --threads 4 --bgen /test/" + tarball_prefix + "." + chromosome + ".BOLT.bgen 'ref-last' " \
             "--sample /test/" + tarball_prefix + "." + chromosome + ".BOLT.sample " \
             "--update-name /test/" + tarball_prefix + "." + chromosome + ".fixer " \
             "--export bgen-1.2 'bits='8 " \
             "--out /test/" + tarball_prefix + "." + chromosome + " " \
             "--keep-fam /test/SAMPLES_Include.txt"
    run_cmd(cmd, True)


# This handles download per-marker information
def process_bolt_markers(chromosome: str, chrom_bgen_index: dict) -> None:

    print("Processing chromosome %s" % chromosome)
    # And then the markers...
    # First we have to download the actual data
    bgen_index = dxpy.DXFile(chrom_bgen_index['index'])
    bgen_sample = dxpy.DXFile(chrom_bgen_index['sample'])
    bgen = dxpy.DXFile(chrom_bgen_index['bgen'])
    vep = dxpy.DXFile(chrom_bgen_index['vep'])
    dxpy.download_dxfile(bgen_index.get_id(), "filtered_bgen/" + chromosome + ".filtered.bgen.bgi")
    dxpy.download_dxfile(bgen_sample.get_id(), "filtered_bgen/" + chromosome + ".filtered.sample")
    dxpy.download_dxfile(bgen.get_id(), "filtered_bgen/" + chromosome + ".filtered.bgen")
    dxpy.download_dxfile(vep.get_id(), "filtered_bgen/" + chromosome + ".filtered.vep.tsv.gz")

    # And then do the filtering...
    cmd = "plink2 --threads 4 --bgen /test/filtered_bgen/" + chromosome + ".filtered.bgen 'ref-last' " \
            "--sample /test/filtered_bgen/" + chromosome + ".filtered.sample " \
            "--export bgen-1.2 'bits='8 " \
            "--out /test/" + chromosome + ".markers " \
            "--keep /test/SAMPLES_Include.txt"
    run_cmd(cmd, True)

    # The sample file output by plink2 is a disaster, so fix it here:
    os.rename(chromosome + '.markers.sample', chromosome + '.old')
    with open(chromosome + '.old', 'r') as samp_file:
        fixed_samp = open(chromosome + '.markers.sample', 'w')
        for line in samp_file:
            line = line.rstrip().split(" ")
            if line[0] == 'ID_1':
                fixed_samp.write('ID_1 ID_2 missing sex\n')
            elif line[3] == 'D':
                fixed_samp.write('0 0 0 D\n')
            else:
                fixed_samp.write("%s %s 0 NA\n" % (line[1], line[1]))
        samp_file.close()
        fixed_samp.close()
    print("Done processing chromosome %s" % chromosome)


# This parses the BOLT output file into a useable format for plotting/R
def process_bolt_outputs(association_pack: dict) -> None:

    # First read in the BOLT stats file:
    bolt_table = pd.read_csv(gzip.open(association_pack['output_prefix'] + '.bgen.stats.gz', 'rt'), sep = "\t")

    # Split the main table into marker and gene tables and remove the larger table
    bolt_table_gene = bolt_table[bolt_table['SNP'].str.contains('ENST')]
    bolt_table_marker = bolt_table[bolt_table['SNP'].str.contains(':')]
    del bolt_table

    # Now process the gene table into a useable format:
    # First read in the transcripts file
    transcripts_table = pd.read_csv(gzip.open('transcripts.tsv.gz', 'rt'), sep = "\t")
    transcripts_table = transcripts_table.rename(columns={'#chrom':'chrom'})
    transcripts_table = transcripts_table.set_index('ENST')
    transcripts_table = transcripts_table[transcripts_table['fail'] == False]
    transcripts_table = transcripts_table.drop(columns=['syn.count','fail.cat','fail'])

    # Test what columns we have in the 'SNP' field so we can name them...
    field_one = bolt_table_gene.iloc[1]
    field_one = field_one['SNP'].split("-")
    field_names = ['ENST']
    if len(field_one) == 2: # This is the bare minimum, always name first column ENST, and second column 'var1'
        field_names.append('var1')
    elif len(field_one) == 3: # This could be the standard naming format... check that column [2] is MAF
        if 'MAF' in field_one[2] or 'AC' in field_one[2]:
            field_names.extend(['MASK','MAF'])
        else: # This means we didn't hit on MAF in column [2] and a different naming convention is used...
            field_names.extend(['var1','var2'])
    else:
        for i in range(2,len(field_one) + 1):
            field_names.append('var%i' % i)

    # Process the 'SNP' column into separate fields and remove
    bolt_table_gene[field_names] = bolt_table_gene['SNP'].str.split("-",expand = True)
    bolt_table_gene = bolt_table_gene.drop(columns=['SNP','CHR','BP','ALLELE1','ALLELE0','GENPOS'])

    # We need to add in an 'AC' column. Pull samples total from the BOLT log file:
    n_bolt = 0
    with open(association_pack['output_prefix'] + '.BOLT.log', 'r') as bolt_log_file:
        for line in bolt_log_file:
            if 'samples (Nbgen):' in line:
                n_bolt = int(line.strip('samples (Nbgen): '))
                break
        bolt_log_file.close()
    # And use them to calculate a MAC
    bolt_table_gene['AC'] = bolt_table_gene['A1FREQ'] * (n_bolt*2)
    bolt_table_gene['AC'] = bolt_table_gene['AC'].round()

    # Now merge the transcripts table into the gene table to add annotation and the write
    bolt_table_gene = pd.merge(transcripts_table, bolt_table_gene, on='ENST', how="left")
    with open(association_pack['output_prefix'] + '.genes.BOLT.stats.tsv', 'w') as gene_out:
        # Sort by chrom/pos just to be sure...
        bolt_table_gene = bolt_table_gene.sort_values(by=['chrom','start','end'])

        bolt_table_gene.to_csv(path_or_buf=gene_out, index = False, sep="\t", na_rep='NA')
        gene_out.close()

        # And bgzip and tabix...
        cmd = "bgzip /test/" + association_pack['output_prefix'] + '.genes.BOLT.stats.tsv'
        run_cmd(cmd, True)
        cmd = "tabix -S 1 -s 2 -b 3 -e 4 /test/" + association_pack['output_prefix'] + '.genes.BOLT.stats.tsv.gz'
        run_cmd(cmd, True)

    # And now process the SNP file (if necessary):
    # Read in the variant index (per-chromosome and mash together)
    if association_pack['run_marker_tests']:
        variant_index = []
        # Open all chromosome indicies and load them into a list and append them together
        for chromosome in CHROMOSOMES:
            variant_index.append(pd.read_csv(gzip.open("filtered_bgen/" + chromosome + ".filtered.vep.tsv.gz", 'rt'), sep = "\t"))

        variant_index = pd.concat(variant_index)
        variant_index = variant_index.set_index('varID')

        # For markers, we can use the SNP ID column to get what we need
        bolt_table_marker = bolt_table_marker.rename(columns={'SNP':'varID', 'A1FREQ':'BOLT_MAF'})
        bolt_table_marker = bolt_table_marker.drop(columns=['CHR','BP','ALLELE1','ALLELE0','GENPOS'])
        bolt_table_marker['BOLT_AC'] = bolt_table_marker['BOLT_MAF'] * (n_bolt*2)
        bolt_table_marker['BOLT_AC'] = bolt_table_marker['BOLT_AC'].round()
        bolt_table_marker = pd.merge(variant_index, bolt_table_marker, on='varID', how="left")
        with open(association_pack['output_prefix'] + '.markers.BOLT.stats.tsv', 'w') as marker_out:
            # Sort by chrom/pos just to be sure...
            bolt_table_marker = bolt_table_marker.sort_values(by=['CHROM','POS'])

            bolt_table_marker.to_csv(path_or_buf=marker_out, index = False, sep="\t", na_rep='NA')
            gene_out.close()

            # And bgzip and tabix...
            cmd = "bgzip /test/" + association_pack['output_prefix'] + '.markers.BOLT.stats.tsv'
            run_cmd(cmd, True)
            cmd = "tabix -S 1 -s 2 -b 3 -e 3 /test/" + association_pack['output_prefix'] + '.markers.BOLT.stats.tsv.gz'
            run_cmd(cmd, True)


# Run rare variant association testing using BOLT
def bolt(association_pack: dict) -> None:

    # Need to pare down the bgen file to samples being tested
    # Have to do this for all chromosomes and all included tarballs, so going to parallelise:
    available_workers = math.floor((association_pack['threads'] - 1) / 4)
    executor = ThreadPoolExecutor(max_workers=available_workers)

    future_pool = []
    with open('poss_chromosomes.txt', 'w') as poss_chromosomes:
        for chromosome in CHROMOSOMES:
            for tarball_prefix in association_pack['tarball_prefixes']:
                if exists(tarball_prefix + "." + chromosome + ".BOLT.bgen"):
                    poss_chromosomes.write("/test/%s /test/%s\n" % (tarball_prefix + "." + chromosome + ".bgen", tarball_prefix + "." + chromosome + ".sample"))
                    future_pool.append(executor.submit(process_bolt_file,
                                                       tarball_prefix = tarball_prefix,
                                                       chromosome = chromosome))

            if association_pack['run_marker_tests']:
                poss_chromosomes.write("/test/%s /test/%s\n" % (chromosome + ".markers.bgen", chromosome + ".markers.sample"))
                chrom_bgen_index = association_pack['bgen_dict'][chromosome] # This holds the information for downloading the bgen file
                future_pool.append(executor.submit(process_bolt_markers,
                                                   chromosome = chromosome,
                                                   chrom_bgen_index = chrom_bgen_index))


        poss_chromosomes.close()
        for future in futures.as_completed(future_pool):
            try:
                future.result()
            except Exception as err:
                print("A BOLT processing thread failed...")
                print(Exception, err)
                raise dxpy.AppError("A BOLT processing thread failed...")

    # See the README.md for more information on these parameters
    cmd = "bolt " + \
          "--bfile=/test/genetics/UKBB_450K_Autosomes_QCd_WBA " \
          "--exclude=/test/genetics/UKBB_450K_Autosomes_QCd.low_MAC.snplist " \
          "--phenoFile=/test/phenotypes_covariates.formatted.txt " \
          "--phenoCol=" + association_pack['pheno_name'] + " " \
          "--covarFile=/test/phenotypes_covariates.formatted.txt " \
          "--covarCol=sex " \
          "--covarCol=wes_batch " \
          "--qCovarCol=age " \
          "--qCovarCol=age_squared " \
          "--qCovarCol=PC{1:10} " \
          "--covarMaxLevels=110 " \
          "--LDscoresFile=BOLT-LMM_v2.3.6/tables/LDSCORE.1000G_EUR.tab.gz " \
          "--geneticMapFile=BOLT-LMM_v2.3.6/tables/genetic_map_hg19_withX.txt.gz " \
          "--lmmInfOnly " \
          "--numThreads=" + str(association_pack['threads']) + " " \
          "--statsFile=/test/" + association_pack['output_prefix'] + ".stats.gz " \
          "--verboseStats " \
          "--bgenSampleFileList=/test/poss_chromosomes.txt " \
          "--statsFileBgenSnps=/test/" + association_pack['output_prefix'] + ".bgen.stats.gz"
    if len(association_pack['quant_covars']) > 0:
        for covar in association_pack['quant_covars']:
            cmd += " --qCovarCol=" + covar + " "
    if len(association_pack['cat_covars']) > 0:
        for covar in association_pack['cat_covars']:
            cmd += " --covarCol=" + covar + " "
    run_cmd(cmd, True, association_pack['output_prefix'] + ".BOLT.log")

    # And process the outputs...
    process_bolt_outputs(association_pack)


# Run rare variant association testing using SAIGE-GENE
def saige_step_one(association_pack: dict) -> None:

    # See the README.md for more information on these parameters
    cmd = "step1_fitNULLGLMM.R " \
          "--plinkFile=/test/genetics/UKBB_450K_Autosomes_QCd_WBA " \
          "--phenoFile=/test/phenotypes_covariates.formatted.txt " \
          "--phenoCol=" + association_pack['pheno_name'] + " " \
          "--isCovariateTransform=FALSE " \
          "--sampleIDColinphenoFile=IID " \
          "--outputPrefix=/test/" + association_pack['pheno_name'] + ".SAIGE_OUT " \
          "--outputPrefix_varRatio=/test/" + association_pack['pheno_name'] + ".SAIGE_OUT_cate " \
          "--sparseGRMFile=/test/genetics/sparseGRM_450K_Autosomes_QCd_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseGRM.mtx " \
          "--sparseGRMSampleIDFile=/test/genetics/sparseGRM_450K_Autosomes_QCd_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseGRM.mtx.sampleIDs.txt " \
          "--nThreads=" + str(association_pack['threads']) + " " \
          "--LOCO=FALSE " \
          "--skipModelFitting=FALSE " \
          "--IsSparseKin=TRUE " \
          "--isCateVarianceRatio=TRUE " \
          "--useSparseGRMtoFitNULL=FALSE "
    if association_pack['sex'] == 2:
        if len(association_pack['quant_covars']) > 0:
            quant_covars_join = ','.join(association_pack['quant_covars'])
            cmd = cmd + "--covarColList=PC1,PC2,PC3,PC4,PC5,PC6,PC7,PC8,PC9,PC10,age,age_squared,sex," + quant_covars_join + " "
        else:
            cmd = cmd + "--covarColList=PC1,PC2,PC3,PC4,PC5,PC6,PC7,PC8,PC9,PC10,age,age_squared,sex "
    else:
        if len(association_pack['quant_covars']) > 0:
            quant_covars_join = ','.join(association_pack['quant_covars'])
            cmd = cmd + "--covarColList=PC1,PC2,PC3,PC4,PC5,PC6,PC7,PC8,PC9,PC10,age,age_squared," + quant_covars_join + " "
        else:
            cmd = cmd + "--covarColList=PC1,PC2,PC3,PC4,PC5,PC6,PC7,PC8,PC9,PC10,age,age_squared "
    if association_pack['is_binary']:
        cmd = cmd + "--traitType=binary"
    else:
        cmd = cmd + "--traitType=quantitative"

    run_cmd(cmd, True, association_pack['pheno_name'] + ".SAIGE_step1.log")


# This is a helper function to parallelise SAIGE step 2 by chromosome
# This returns the tarball_prefix and chromosome number to make it easier to generate output
def saige_step_two(tarball_prefix: str, chromosome: str, pheno_name: str, run_marker_tests: bool) -> tuple:

    cmd = "bcftools view --threads 1 -S /test/SAMPLES_Include.txt -Oz -o /test/" + tarball_prefix + "." + chromosome + ".saige_input.vcf.gz /test/" + tarball_prefix + "." + chromosome + ".SAIGE.bcf"
    run_cmd(cmd, True)
    cmd = "bcftools index --threads 1 /test/" + tarball_prefix + "." + chromosome + ".saige_input.vcf.gz"
    run_cmd(cmd, True)

    # See the README.md for more information on these parameters
    cmd = "step2_SPAtests.R " \
          "--vcfFile=/test/" + tarball_prefix + "." + chromosome + ".saige_input.vcf.gz " \
          "--vcfField=GT " \
          "--GMMATmodelFile=/test/" + pheno_name + ".SAIGE_OUT.rda " \
          "--varianceRatioFile=/test/" + pheno_name + ".SAIGE_OUT_cate.varianceRatio.txt " \
          "--sparseSigmaFile=/test/" + pheno_name + ".SAIGE_OUT_cate.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx " \
          "--LOCO=FALSE " \
          "--SAIGEOutputFile=/test/" + tarball_prefix + "." + chromosome + ".SAIGE_OUT.SAIGE.gene.txt " \
          "--groupFile=/test/" + tarball_prefix + "." + chromosome + ".SAIGE.groupFile.txt " \
          "--MACCutoff_to_CollapseUltraRare=0.5 " \
          "--IsOutputHetHomCountsinCaseCtrl=TRUE " \
          "--IsOutputNinCaseCtrl=TRUE " \
          "--maxMAFforGroupTest=1"
    if run_marker_tests:
        cmd += " --IsSingleVarinGroupTest=TRUE"
    run_cmd(cmd, True, tarball_prefix + "." + chromosome + ".SAIGE_step2.log")

    return tarball_prefix, chromosome


# Generate the NULL model for STAAR
def staar_null(association_pack: dict) -> None:

    # I have made a custom script in order to generate the STAAR Null model:
    # located in /usr/bin/runSTAAR_Null.R
    # This generates an RDS output file containing the NULL model
    # See the README.md for more information on these parameters
    cmd = "Rscript /prog/runSTAAR_Null.R " \
          "/test/phenotypes_covariates.formatted.txt " + \
          association_pack['pheno_name'] + " " + \
          str(association_pack['is_binary'])
    if len(association_pack['quant_covars']) > 0:
        cmd += " " + ','.join(association_pack['quant_covars'])
    else:
        cmd += " NULL"
    if len(association_pack['cat_covars']) > 0:
        cmd += " " + ','.join(association_pack['cat_covars'])
    else:
        cmd += " NULL"
    run_cmd(cmd, True)


# Run rare variant association testing using STAAR
# Returns the finished chromosome to aid in output file creation
def staar_genes(tarball_prefix: str, chromosome: str, pheno_name: str) -> tuple:

    # I have made a custom script in order to run the actual per-gene model in STAAR:
    # located in /usr/bin/runSTAAR_Genes.R
    # This generates a text output file of p.values
    # See the README.md for more information on these parameters
    print("Chromosome %s for mask %s started STAAR." % (chromosome, tarball_prefix))

    cmd = "Rscript /prog/runSTAAR_Genes.R " \
          "/test/" + tarball_prefix + "." + chromosome + ".STAAR.matrix.rds " \
          "/test/" + tarball_prefix + "." + chromosome + ".variants_table.STAAR.tsv " \
          "/test/" + pheno_name + ".STAAR_null.rds " + \
          pheno_name + " " + \
          tarball_prefix + " " + \
          chromosome
    run_cmd(cmd, True)

    return tarball_prefix, chromosome


# Process STAAR output files
def process_staar_outputs(completed_staar_files: list, association_pack: dict) -> None:

    # Here we are concatenating a temp file of each tsv from completed_staar_files:
    with open(association_pack['output_prefix'] + '.genes.STAAR.stats.temp', 'w') as staar_output:
        output_csv = csv.DictWriter(staar_output,
                                    delimiter = "\t",
                                    fieldnames=['SNP','n.samps','pheno','staar.O.p','staar.SKAT.p','staar.burden.p',
                                                'staar.ACAT.p','n.var','cMAC'])
        output_csv.writeheader()
        for file in completed_staar_files:
            with open(file, 'r') as curr_file_reader:
                curr_file_csv = csv.DictReader(curr_file_reader, delimiter = "\t")
                for gene in curr_file_csv:
                    output_csv.writerow(gene)
            curr_file_reader.close()
        staar_output.close()

    # Now read in the concatenated STAAR stats file:
    staar_table = pd.read_csv(open(association_pack['output_prefix'] + '.genes.STAAR.stats.temp', 'r'), sep = "\t")

    # Now process the gene table into a useable format:
    # First read in the transcripts file
    transcripts_table = pd.read_csv(gzip.open('transcripts.tsv.gz', 'rt'), sep = "\t")
    transcripts_table = transcripts_table.rename(columns={'#chrom':'chrom'})
    transcripts_table = transcripts_table.set_index('ENST')
    transcripts_table = transcripts_table[transcripts_table['fail'] == False]
    transcripts_table = transcripts_table.drop(columns=['syn.count','fail.cat','fail'])

    # Test what columns we have in the 'SNP' field so we can name them...
    field_one = staar_table.iloc[1]
    field_one = field_one['SNP'].split("-")
    field_names = ['ENST']
    if len(field_one) == 2: # This is the bare minimum, always name first column ENST, and second column 'var1'
        field_names.append('var1')
    elif len(field_one) == 3: # This could be the standard naming format... check that column [2] is MAF/AC
        if 'MAF' in field_one[2] or 'AC' in field_one[2]:
            field_names.extend(['MASK','MAF'])
        else: # This means we didn't hit on MAF in column [2] and a different naming convention is used...
            field_names.extend(['var1','var2'])
    else:
        for i in range(2,len(field_one) + 1):
            field_names.append('var%i' % i)

    # Process the 'SNP' column into separate fields and remove
    staar_table[field_names] = staar_table['SNP'].str.split("-",expand = True)
    staar_table = staar_table.drop(columns=['SNP'])

    # Now merge the transcripts table into the gene table to add annotation and the write
    staar_table = pd.merge(transcripts_table, staar_table, on='ENST', how="left")
    with open(association_pack['output_prefix'] + '.genes.STAAR.stats.tsv', 'w') as gene_out:

        # Sort just in case
        staar_table = staar_table.sort_values(by=['chrom','start','end'])

        staar_table.to_csv(path_or_buf=gene_out, index = False, sep="\t", na_rep='NA')
        gene_out.close()

        # And bgzip and tabix...
        cmd = "bgzip /test/" + association_pack['output_prefix'] + '.genes.STAAR.stats.tsv'
        run_cmd(cmd, True)
        cmd = "tabix -S 1 -s 2 -b 3 -e 4 /test/" + association_pack['output_prefix'] + '.genes.STAAR.stats.tsv.gz'
        run_cmd(cmd, True)


# Setup linear models:
# I really don't like how I supply the association_pack here, but I think easiest way to do it...
def linear_model_null(association_pack: dict) -> dict:

    # load covariates and phenotypes
    pheno_covars = pd.read_csv("phenotypes_covariates.formatted.txt",
                               sep=" ",
                               index_col="FID",
                               dtype={'IID':str})
    pheno_covars.index = pheno_covars.index.astype(str)

    # Decide what model family to use:
    if association_pack['is_binary']:
        family = sm.families.Binomial()
    else:
        family = sm.families.Gaussian()

    # And finally define the formula to be used by all models:
    # Make sure to define additional covariates as requested by the user...
    form_null = association_pack['pheno_name'] + ' ~ sex + age + age_squared + C(wes_batch) + PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 + PC8 + PC9 + PC10'
    if len(association_pack['quant_covars']) > 0:
        for covar in association_pack['quant_covars']:
            form_null += ' + ' + covar
    if len(association_pack['cat_covars']) > 0:
        for covar in association_pack['cat_covars']:
            form_null += ' + C(' + covar + ')'

    form_full = form_null + ' + has_var'

    print('Using the following formula for GLMs: ')
    print(form_full)

    # Build the null model and extract residuals:
    print('fitting NULL model')
    sm_results_null = sm.GLM.from_formula(form_null, data=pheno_covars, family=sm.families.Gaussian()).fit()
    null_table = sm_results_null.resid_response.to_frame()
    null_table = null_table.rename(columns={0:'resid'})

    # Start building a set of models we want to test.
    linear_model_pack = {'phenotypes': pheno_covars,
                         'pheno_name': association_pack['pheno_name'],
                         'model_family': family,
                         'model_formula': form_full,
                         'n_model': len(pheno_covars),
                         'null_model': null_table,
                         'genotypes': {}} # 'genotypes' is an empty dict that we will fill with individual mask tables

    return linear_model_pack


# load genes/genetic data we want to test/use:
# For each tarball prefix, we want to make ONE dict for efficient querying of variants
def load_tarball_linear_model(tarball_prefix: str) -> tuple:

    print("Loading tarball prefix: " + tarball_prefix)
    geno_tables = []
    for chromosome in CHROMOSOMES:
        # This handles the genes that we need to test:
        if exists(tarball_prefix + "." + chromosome + ".BOLT.bgen"):
            # This handles the actual genetic data:
            # load genetic data
            # first convert into format we can use:
            cmd = "plink2 --threads 2 --bgen /test/" + tarball_prefix + "." + chromosome + ".BOLT.bgen 'ref-last' " \
                          "--sample /test/" + tarball_prefix + "." + chromosome + ".BOLT.sample " \
                          "--export bcf --out /test/lm." + tarball_prefix + "." + chromosome + " " + \
                          "--keep-fam /test/SAMPLES_Include.txt"
            run_cmd(cmd, True)

            # This just makes a sparse matrix with columns: sample_id, gene name, genotype
            cmd = "bcftools query -i \"GT='alt'\" -f \"[%SAMPLE\\t%ID\\t%GT\\n]\" " \
                          "/test/lm." + tarball_prefix + "." + chromosome + ".bcf > " \
                          "lm." + tarball_prefix + "." + chromosome + ".tsv"
            run_cmd(cmd, True)

            geno_table = pd.read_csv("lm." + tarball_prefix + "." + chromosome + ".tsv",
                                     sep = "\t",
                                     names = ['eid', 'gene', 'gt'])
            # bgen stores samples in eid_eid format, so get just the first eid
            geno_table[['eid','eid2']] = geno_table['eid'].str.split('_', 1, expand=True)
            geno_table = geno_table.drop('eid2', axis=1)

            # Get all possible genes found and aggregate across gene IDs to get a list of individuals with a given gene
            geno_table = geno_table.groupby('gene').agg({'eid': Series.to_list})
            geno_tables.append(geno_table)

    # And concatenate the final data_frame together:
    genetic_data = pd.concat(geno_tables).to_dict()
    genetic_data = genetic_data['eid'] # no idea why there is a top level 'eid' key
    print("Finished loading tarball prefix: " + tarball_prefix)

    return tarball_prefix, genetic_data


# Run rare variant association testing using GLMs
def linear_model_genes(linear_model_pack: dict, gene: str, mask_name: str) -> dict:

    # Now successively iterate through each gene and run our model:
    # I think this is straight-forward?
    try:
        indv_w_var = linear_model_pack['genotypes'][mask_name][gene]

        # We have to make an internal copy as this DataFrame is NOT threadsafe...
        internal_frame = pd.DataFrame.copy(linear_model_pack['null_model'])
        internal_frame['has_var'] = np.where(internal_frame.index.isin(indv_w_var), 1, 0)
        n_car = len(internal_frame.loc[internal_frame['has_var'] == 1])
        if n_car <= 2:
            gene_dict = {'p_val_init': 'NA',
                         'n_car': n_car,
                         'n_model': linear_model_pack['n_model'],
                         'ENST': gene,
                         'maskname': mask_name,
                         'pheno_name': linear_model_pack['pheno_name'],
                         'p_val_full': 'NA',
                         'effect': 'NA',
                         'std_err': 'NA'}
        else:
            sm_results = sm.GLM.from_formula('resid ~ has_var',
                                             data=internal_frame,
                                             family=sm.families.Gaussian()).fit()
            # If we get a significant result here, re-test with the full model to get accurate beta/p. value/std. err.
            if sm_results.pvalues['has_var'] < 1e-4:

                internal_frame = pd.DataFrame.copy(linear_model_pack['phenotypes'])
                internal_frame['has_var'] = np.where(internal_frame.index.isin(indv_w_var), 1, 0)

                sm_results_full = sm.GLM.from_formula(linear_model_pack['model_formula'],
                                                      data=internal_frame,
                                                      family=linear_model_pack['model_family']).fit()
                gene_dict = {'p_val_init': sm_results.pvalues['has_var'],
                             'n_car': n_car,
                             'n_model': sm_results.nobs,
                             'ENST': gene,
                             'maskname': mask_name,
                             'pheno_name': linear_model_pack['pheno_name'],
                             'p_val_full': sm_results_full.pvalues['has_var'],
                             'effect': sm_results_full.params['has_var'],
                             'std_err': sm_results_full.bse['has_var']}
            else:
                gene_dict = {'p_val_init': sm_results.pvalues['has_var'],
                             'n_car': n_car,
                             'n_model': sm_results.nobs,
                             'ENST': gene,
                             'maskname': mask_name,
                             'pheno_name': linear_model_pack['pheno_name'],
                             'p_val_full': 'NA',
                             'effect': 'NA',
                             'std_err': 'NA'}
        return gene_dict

    except Exception as err:
        print(Exception, err)
        raise Exception("Gene " + gene + " failed to run properly...")


# Process/annotate Linear Model output file
def process_linear_model_outputs(association_pack: dict):

    # read in the GLM stats file:
    glm_table = pd.read_csv(open(association_pack['output_prefix'] + ".lm_stats.tmp", 'r'), sep = "\t")

    # Now process the gene table into a useable format:
    # First read in the transcripts file
    transcripts_table = pd.read_csv(gzip.open('transcripts.tsv.gz', 'rt'), sep = "\t")
    transcripts_table = transcripts_table.rename(columns={'#chrom':'chrom'})
    transcripts_table = transcripts_table.set_index('ENST')
    transcripts_table = transcripts_table[transcripts_table['fail'] == False]
    transcripts_table = transcripts_table.drop(columns=['syn.count','fail.cat','fail'])

    # Test what columns we have in the 'SNP' field so we can name them...
    field_one = glm_table.iloc[1]
    field_one = field_one['maskname'].split("-")
    field_names = []
    if len(field_one) == 2: # This could be the standard naming format... check that column [1] is MAF/AC
        if 'MAF' in field_one[1] or 'AC' in field_one[1]:
            field_names.extend(['MASK','MAF'])
        else: # This means we didn't hit on MAF/AC in column [2] and a different naming convention is used...
            field_names.extend(['var1','var2'])
    else:
        for i in range(2,len(field_one) + 1):
            field_names.append('var%i' % i)

    # Process the 'SNP' column into separate fields and remove
    glm_table[field_names] = glm_table['maskname'].str.split("-",expand = True)
    glm_table = glm_table.drop(columns=['maskname'])

    # Now merge the transcripts table into the gene table to add annotation and the write
    glm_table = pd.merge(transcripts_table, glm_table, on='ENST', how="left")
    with open(association_pack['output_prefix'] + '.genes.glm.stats.tsv', 'w') as gene_out:

        # Sort just in case
        glm_table = glm_table.sort_values(by=['chrom','start','end'])

        glm_table.to_csv(path_or_buf=gene_out, index = False, sep="\t", na_rep='NA')
        gene_out.close()

        # And bgzip and tabix...
        cmd = "bgzip /test/" + association_pack['output_prefix'] + '.genes.glm.stats.tsv'
        run_cmd(cmd, True)
        cmd = "tabix -S 1 -s 2 -b 3 -e 4 /test/" + association_pack['output_prefix'] + '.genes.glm.stats.tsv.gz'
        run_cmd(cmd, True)


@dxpy.entry_point('main')
def main(association_tarballs, run_bolt, run_staar, run_saige, run_linear_model, run_all, is_binary, sex, exclusion_list,
         inclusion_list, phenofile, covarfile, categorical_covariates, quantitative_covariates, output_prefix, run_marker_tests,
         bgen_index, transcript_index, base_covariates, bed_file, fam_file, bim_file, low_MAC_list, sparse_grm, sparse_grm_sample):

    # First get number of cores available to the instance:
    threads = os.cpu_count()
    print('Number of threads available: %i' % threads)

    if run_bolt is False and run_staar is False and run_saige is False and run_linear_model is False and run_all is False:
        raise Exception("No models were selected for execution!")

    # Bring our docker image into our environment so that we can run commands we need:
    cmd = "docker pull egardner413/mrcepid-associationtesting:latest"
    run_cmd(cmd)

    # Grab the data necessary to run this:
    injested_data_info = ingest_data(association_tarballs, phenofile, covarfile, inclusion_list, exclusion_list,
                                     run_bolt, bgen_index, transcript_index, base_covariates,
                                     bed_file, fam_file, bim_file, low_MAC_list, sparse_grm, sparse_grm_sample)

    # This does sample and covariate processing for all pipelines regardless of what we need to run
    # Also returns the phenotype name we are going to test
    genetics_samples = select_individuals(injested_data_info['inclusion_found'],
                                          injested_data_info['exclusion_found'])
    association_pack = create_covariate_file(sex,
                                             genetics_samples,
                                             injested_data_info['additional_covariates_found'],
                                             categorical_covariates,
                                             quantitative_covariates)
    print("Phenotype: " + association_pack['pheno_name'])

    # Attach additional required information to the 'association_pack' variable to enable easy function running:
    association_pack['tarball_prefixes'] = injested_data_info['tarball_prefixes']
    association_pack['bgen_dict'] = injested_data_info['bgen_dict']
    association_pack['is_binary'] = is_binary
    association_pack['sex'] = sex
    association_pack['threads'] = threads
    association_pack['run_marker_tests'] = run_marker_tests
    association_pack['output_prefix'] = output_prefix

    # Run models that were selected  first checking if 'run_all' is enabled:
    output_files = [] # Create a list of outputs to drop into the tarball later
    if run_all:
        run_bolt = True
        run_saige = True
        run_staar = True
        run_linear_model = True

    # Then run selected models
    # Set up a thread pool executor to parallelise by chromosome where required:
    # Need to pare down tested samples to those in the covariate file using bcftools:
    # Running each chromosome on a separate thread to speed things up
    available_workers = association_pack['threads'] - 1
    executor = ThreadPoolExecutor(max_workers=available_workers)

    if run_bolt:
        print("Running BOLT")
        bolt(association_pack)
        output_files.append(association_pack['output_prefix'] + ".stats.gz")
        output_files.append(association_pack['output_prefix'] + '.genes.BOLT.stats.tsv.gz')
        output_files.append(association_pack['output_prefix'] + '.genes.BOLT.stats.tsv.gz.tbi')
        output_files.append(association_pack['output_prefix'] + ".BOLT.log")
        if association_pack['run_marker_tests']:
            output_files.append(association_pack['output_prefix'] + '.markers.BOLT.stats.tsv.gz')
            output_files.append(association_pack['output_prefix'] + '.markers.BOLT.stats.tsv.gz.tbi')
    if run_saige:
        # Run step one without parallelisation
        print("Running SAIGE step 1...")
        saige_step_one(association_pack)
        output_files.append(association_pack['pheno_name'] + ".SAIGE_step1.log") # Add step1 log to the tarball

        # Run step two WITH parallelisation by chromosome
        print("Running SAIGE step 2...")
        future_pool = []
        for chromosome in ['22']:
            for tarball_prefix in association_pack['tarball_prefixes']:
                if exists(tarball_prefix + "." + chromosome + ".SAIGE.bcf"):
                    future_pool.append(executor.submit(saige_step_two,
                                                       tarball_prefix = tarball_prefix,
                                                       chromosome = chromosome,
                                                       pheno_name = association_pack['pheno_name'],
                                                       run_marker_tests = association_pack['run_marker_tests']))

        # And gather the resulting futures
        completed_saige_files = []
        completed_saige_marker_files = []
        for future in futures.as_completed(future_pool):
            try:
                tarball_prefix, finished_chromosome = future.result()
                print("\nChromosome %s for mask %s finished SAIGE step2." % (finished_chromosome, tarball_prefix))
                output_files.append(tarball_prefix + "." + finished_chromosome + ".SAIGE_step2.log") # Include the log file for each tarball/chromosome in output
                output_files.append(tarball_prefix + "." + finished_chromosome + ".SAIGE_OUT.SAIGE.gene.txt")
                if association_pack['run_marker_tests']:
                    output_files.append(tarball_prefix + "." + finished_chromosome + ".SAIGE_OUT.SAIGE.gene.txt_single")
            except Exception as err:
                print(Exception, err)
                raise dxpy.AppError("A SAIGE thread failed...")
    if run_staar:
        print("Running STAAR")
        print("Running STAAR Null Model")
        staar_null(association_pack)

        print("Running STAAR masks * chromosomes")
        future_pool = []
        total_models = 0
        for chromosome in CHROMOSOMES:
            for tarball_prefix in association_pack['tarball_prefixes']:
                if exists(tarball_prefix + "." + chromosome + ".STAAR.matrix.rds"):
                    total_models+=1
                    future_pool.append(executor.submit(staar_genes,
                                                       tarball_prefix = tarball_prefix,
                                                       chromosome = chromosome,
                                                       pheno_name = association_pack['pheno_name']))
        print("Total number of chromosome / mask pairs to run: %i" % total_models)

        # And gather the resulting futures
        completed_staar_files = []
        total_finished_models = 0
        for future in futures.as_completed(future_pool):
            try:
                tarball_prefix, finished_chromosome = future.result()
                print("Chromosome %s for mask %s finished STAAR." % (finished_chromosome, tarball_prefix))
                completed_staar_files.append(tarball_prefix + "." + finished_chromosome + ".STAAR_results.tsv")
                total_finished_models += 1
                if math.remainder(total_finished_models, 10) == 0:
                    prop_finished = (total_finished_models / total_models) * 100
                    print("Total number of chromosome / mask pairs finished: %i / %i (%0.2f%%)" % (total_finished_models, total_models, prop_finished))
            except Exception as err:
                print(Exception, err)
                raise dxpy.AppError("A STAAR thread failed...")

        process_staar_outputs(completed_staar_files, association_pack)

        # Add the final models and null model to the output...
        output_files.append(association_pack['output_prefix'] + '.genes.STAAR.stats.tsv.gz')
        output_files.append(association_pack['output_prefix'] + '.genes.STAAR.stats.tsv.gz.tbi')
        os.rename(association_pack['pheno_name'] + ".STAAR_null.rds", association_pack['output_prefix'] + ".STAAR_null.rds") # I know, this is lazy...
        output_files.append(association_pack['output_prefix'] + ".STAAR_null.rds")
    if run_linear_model:
        print("Running Linear Models")

        # First, do setup for the linear models.
        # This will load all variants, genes, and phenotypes into memory to allow for parallelization
        # This function returns a dictionary with the following keys:
        # 'genes': The list of genes to iterate over
        # 'phenotypes': Phenotypes/covariates for every individual
        # 'genotypes': Sparse matrix of genotypes in pandas data.frame format
        # 'model_formula': Formatted formula for all linear models
        # 'n_model': Number of individuals with values in the pheno/covariate file (do this here to save compute in threads)
        # 'genotypes': A dictionary storing "genotypes" for all masks provided to this tool (empty here, we fill below)
        print("Running Null Linear Model")
        linear_model_pack = linear_model_null(association_pack)

        # And load the tarballs INTO the genotypes dictionary
        print("Loading genotypes")
        future_pool = []
        for tarball_prefix in association_pack['tarball_prefixes']:
            future_pool.append(executor.submit(load_tarball_linear_model,
                                               tarball_prefix = tarball_prefix))

        for future in futures.as_completed(future_pool):
            try:
                tarball_prefix, genotype_dict = future.result()
                linear_model_pack['genotypes'][tarball_prefix] = genotype_dict
            except Exception as err:
                print(Exception, err)
        print("Finished loading genotypes")

        print("Submitting models to threads")
        # Next we are going to iterate through every model / gene (in linear_model_pack['genes']) pair and run a GLM
        future_pool = []
        total_models = 0
        for model in linear_model_pack['genotypes']:
            for gene in linear_model_pack['genotypes'][model]:
                total_models+=1
                future_pool.append(executor.submit(linear_model_genes,
                                                   linear_model_pack = linear_model_pack,
                                                   gene = gene,
                                                   mask_name = model))

        print("Total number of gene / mask pairs to iterate through: %i" % total_models)

        # Write results:
        lm_stats_file = open(association_pack['output_prefix'] + ".lm_stats.tmp", 'w')
        lm_stats_writer = csv.DictWriter(lm_stats_file,
                                         delimiter = "\t",
                                         fieldnames=['ENST','maskname','pheno_name','p_val_init','n_car','n_model','p_val_full','effect','std_err'])
        lm_stats_writer.writeheader()
        total_finished_models = 0
        for future in futures.as_completed(future_pool):
            try:
                finished_gene = future.result()
                lm_stats_writer.writerow(finished_gene)
                total_finished_models += 1
                if math.remainder(total_finished_models, 1000) == 0:
                    prop_finished = (total_finished_models / total_models) * 100
                    print("Total number of gene / mask pairs finished: %i / %i (%0.2f%%)" % (total_finished_models, total_models, prop_finished))
            except Exception as err:
                print(Exception, err)

        process_linear_model_outputs(association_pack)
        output_files.append(association_pack['output_prefix'] + '.genes.glm.stats.tsv.gz')
        output_files.append(association_pack['output_prefix'] + '.genes.glm.stats.tsv.gz.tbi')
        lm_stats_file.close()

    # Create tar of all possible output files
    if output_prefix is None:
        output_tarball = "assoc_stats.tar.gz"
    else:
        output_tarball = output_prefix + ".assoc_stats.tar.gz"

    tar = tarfile.open(output_tarball, "w:gz")
    for file in output_files:
        tar.add(file)
    tar.close()

    ## Have to do 'upload_local_file' to make sure the new file is registered with dna nexus
    output = {"output_tarball": dxpy.dxlink(dxpy.upload_local_file(output_tarball))}

    return output


dxpy.run()
